{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('crypto_data/LTC-USD.csv', names = ['time','low','high','open','close','volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#merging the data of 4 csv files    \n",
    "main_df = pd.DataFrame()\n",
    "ratios =['BTC-USD','LTC-USD','ETH-USD','BCH-USD']\n",
    "for ratio in ratios:\n",
    "    dataset = f'crypto_data/{ratio}.csv'\n",
    "    \n",
    "    df = pd.read_csv(dataset, names=['time','low','high','open','close','volume'])\n",
    "    df.rename(columns={'close':f'{ratio}_close','volume':f'{ratio}_volume'},inplace=True) #inplace is used so we donot need to redefine dataframe\n",
    "    df.set_index('time',inplace=True)\n",
    "    df = df[[f'{ratio}_close',f'{ratio}_volume']]\n",
    "    \n",
    "    #print(df.head())\n",
    "    \n",
    "    if len(main_df)==0:\n",
    "        main_df=df\n",
    "    else:\n",
    "        main_df = main_df.join(df)\n",
    "        \n",
    "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
    "main_df.dropna(inplace=True)        \n",
    "        \n",
    "SEQ_LEN = 60\n",
    "FUTURE_PERIOD_PREDICT = 3\n",
    "RATIO_TO_PREDICT = 'LTC-USD'\n",
    "\n",
    "def classify(current,future):\n",
    "    if float(future)>float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "\n",
    "print(main_df[[f'{RATIO_TO_PREDICT}_close','future']].head())\n",
    "\n",
    "main_df['target'] = list(map(classify,main_df[f'{RATIO_TO_PREDICT}_close'],main_df['future']))\n",
    "\n",
    "print(main_df[[f'{RATIO_TO_PREDICT}_close','future','target']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = sorted(main_df.index.values)\n",
    "last_5pct = times[-int(0.05*len(times))]\n",
    "print(last_5pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "main_df = main_df[(main_df.index < last_5pct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):    \n",
    "    df = df.drop('future',1)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col != 'target':\n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace=True)\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "            \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])\n",
    "            \n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    buys = []\n",
    "    sells = []\n",
    "\n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq,target])\n",
    "\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "\n",
    "    lower = min(len(buys), len(sells))\n",
    "\n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "\n",
    "    sequential_data = buys+sells\n",
    "\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    X =[]\n",
    "    y =[]\n",
    "\n",
    "    for seq,target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y = preprocess_df(main_df)\n",
    "validation_X, validation_y = preprocess_df(validation_main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train data: {len(train_X)} validation: {len(validation_X)}\")\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_X.shape[1:]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape=(train_X.shape[1:]),return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape=(train_X.shape[1:]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001,decay=1e-6) #lr=learning rate\n",
    "#model.compile(loss='mse', optimizer='adam', metrics=['accuracy']\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer = opt,\n",
    "             metrics = ['accuracy'])\n",
    "tensorboard = TensorBoard(log_dir=f'logs\\\\{NAME}')\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_accuracy:.3f}.hdf5\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
    "\n",
    "#checkpoint = ModelCheckpoint('deep-learning-model-full-v0.03.01.weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5' , monitor='val_loss' , verbose=1 , save_best_only=True , period=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.asarray(train_y)\n",
    "validation_y = np.asarray(validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_X, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_X, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"models/{}\".format(NAME))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
